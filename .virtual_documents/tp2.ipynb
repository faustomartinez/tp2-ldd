import matplotlib.pyplot as plt
import seaborn as sns
import seaborn.objects as so
import numpy as np
import pandas as pd

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.datasets import make_blobs
from sklearn.neighbors import NearestNeighbors

from formulaic import Formula, model_matrix

import logging
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
logging.getLogger('tensorflow').setLevel(logging.ERROR)
import tensorflow as tf
import keras











# Guardamos los datos del archivo en el DataFrame FBref
FBref = pd.read_csv("FBRef2020-21.csv")

# Consultamos su cantidad de filas y columnas
FBref.shape





# Nos quedamos con los datos de los jugadores que jugaron más de 500 minutos
FBref = FBref[FBref['Min']>500]

# Verificamos que eliminamos filas
FBref.shape





# Obtenemos una visualización de los datos
FBref


# Vemos cuantos jugadores tienen datos fantantes en cada columna
with pd.option_context("display.max_rows",None):
    print(FBref.isna().sum())


# Definimos nan_cols como las columnas donde la cantidad de NaNs es mas de 50
nan_cols = FBref.isna().sum() > 50

# Definimos keep como las columnas que nos queremos quedar
keep = nan_cols.index[~(nan_cols)]

# Redefinimos a FBref, quedandonos solo con las columnas keep
FBref = FBref[keep]

# Visualizamos el resultado
FBref





# Eliminamos ahora todas las filas donde haya datos faltantes
FBref = FBref.dropna()

# Visualizamos el resultado
FBref





# Verificamos que no hay ninguna columna con datos faltantes
FBref.isna().sum()





# Reseteamos los índices y eliminamos la columna 'index' que tiene los indices anteriores, ya que no la usaremos
FBref = FBref.reset_index().drop('index',axis=1)

# Visualizamos el resultado
FBref





# Obtenemos los nombres de las primeras 20 columnas
FBref.columns[0:20]





# Eliminamos todas las columnas previas a Ast/90
data_num = FBref.drop(FBref.columns[0:11],axis=1)

# Visualizamos el resultado
data_num


# Obtenemos información del dataset data_num
data_num.info()





# Definimos columnas_object como las columnas que son de tipo object
columnas_object = data_num.select_dtypes(include='object')

# Visualizamos dichas columnas
columnas_object














# Visualizamos algunas de las columnas disponibles para elegir
data_num.columns





# Realizamos el gráfico de dispersión pedido
(
    so.Plot(data=data_num,x="LowPass/90",y="Shots/90")
    .add(so.Dot())
)











# Redefinimos data_num como los datos escalados
data_num[data_num.columns] = MinMaxScaler().fit_transform(data_num)

# Observamos el resultado
data_num


# Verificamos que el máximo de cada columna está en 1
data_num.max()


# Iniciamos un PCA indicando que queremos las primeras dos direcciones principales z1 y z2
pca = PCA(n_components = 2)

# Definimos las componentes principales como los datos escalados
componentesPrincipales = pca.fit_transform(data_num)

# Visualizamos el resultado
display(componentesPrincipales)


# Graficamos los datos en función de las componentes principales
(
    so.Plot(x=componentesPrincipales[:,0],y=componentesPrincipales[:,1])
    .add(so.Dot())
    .add(so.Text())
)





# Graficamos en función de las componentes principales, coloreando según la posición
(
    so.Plot(x=componentesPrincipales[:,0],y=componentesPrincipales[:,1],color=FBref.Pos)
    .add(so.Dot())
)








# Inicializamos un KMeans, con 2 clusters: Arqueros y Jugadores de campo
kmeans = KMeans(n_clusters=2)

# Definimos etiquetas1 como la predicción con todos los datos
etiquetas1 = kmeans.fit_predict(data_num)

# Definimos etiquetas2 como la predicción usando solo las componentes principales
etiquetas2 = kmeans.fit_predict(componentesPrincipales)


# Coloreamos usando la predicción con todos los datos
(
    so.Plot(x=componentesPrincipales[:,0],y=componentesPrincipales[:,1], color=etiquetas1.astype("str"))
    .add(so.Dot())
)


# Graficamos según la predicción para las componentes principales
(
    so.Plot(x=componentesPrincipales[:,0],y=componentesPrincipales[:,1], color=etiquetas2.astype("str"))
    .add(so.Dot())
)








# Definimos un NearestNeighbors que mida los dos vecinos más cercanos
neighbors = NearestNeighbors(n_neighbors = 2)

# Lo entrenamos con las componentes principales
neighbors_fit = neighbors.fit(componentesPrincipales)


# Definimos las distancias y los indices como los kneighbors de las componentesPrincipales
distances, indices = neighbors_fit.kneighbors(componentesPrincipales)
distances = distances[:,1]
distances = np.sort(distances, axis = 0)

# En base a esto graficamos buscando el codo que nos ayude a elegir el epsilon
so.Plot(x = np.arange(len(distances)), y = distances).add(so.Line())





# Definimos un objecto DBSCAN con epsilon = 0.2 y minSamples = 8
dbscan = DBSCAN(eps = 0.20, min_samples=8)

# Entrenamos y predecimos usando las componentes principales
etiquetas3 = dbscan.fit_predict(componentesPrincipales)


# Graficamos la clasificación obtenida con DBSCAN
(
    so.Plot(x=componentesPrincipales[:,0],y=componentesPrincipales[:,1], color=etiquetas3.astype("str"))
    .add(so.Dot())
)








# Repetimos el proceso de la vez pasada, el cual ahora es mucho más necesario, ya que no es fácil ver cuál es el epsilon
# o el minSamples adecuado para una muestra de mas de 150 columnas
neighbors = NearestNeighbors(n_neighbors = 2)
neighbors_fit = neighbors.fit(data_num)
distances, indices = neighbors_fit.kneighbors(data_num)
distances = distances[:,1]
distances = np.sort(distances, axis = 0)

so.Plot(x = np.arange(len(distances)), y = distances).add(so.Line())





# Definimos un DBSCAN con epsilon = 1.5 y minSamples = 8
dbscan = DBSCAN(eps = 1.5, min_samples=8)

# Definimos etiquetas4 como las predicciones del modelo DBSCAN utilizando todos los datos
etiquetas4 = dbscan.fit_predict(data_num)


(
    so.Plot(x=componentesPrincipales[:,0],y=componentesPrincipales[:,1], color=etiquetas4.astype("str"))
    .add(so.Dot())
)














# Definimos Pos_filt como la posicion para los jugadores que tienen una sola, y la primera que aparece para los que tienen más de una.
FBref["Pos_filt"]=FBref["Pos"].apply(lambda x: x[:2])


# Visualizamos el resultado
FBref





# Definimos data_clasif como una copia de data_num
data_clasif = data_num.copy()

# Y le agregamos la columna Pos_filt
data_clasif["Pos_filt"] =  FBref["Pos_filt"]

# Visualizamos el resultado
data_clasif


# Dividimos el dataset en datos de entrenamiento y testeo
data_clasif_train, data_clasif_test = train_test_split(data_clasif,test_size=0.2,random_state=42)





from sklearn.neighbors import KNeighborsClassifier


# Definimos las variables predictoras y las predichas
X_train = data_clasif_train.drop("Pos_filt",axis=1)
y_train = data_clasif_train["Pos_filt"]
X_test = data_clasif_test.drop("Pos_filt",axis=1)
y_test = data_clasif_test["Pos_filt"]


# Creamos una lista con los valores de k en los que vamos a evaluar el rendimiento de KNeighborsClassifier
k_values = range (1,21)
scores = []

# Para cada valor de k, vamos guardando la media del score por validación cruzada en una lista scores
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X_train, y_train, cv=5)
    scores.append(np.mean(score))


# Graficamos los scores
(
    so.Plot(x=k_values,y=scores)
    .add(so.Line())
    .add(so.Dot())
).layout(size=(6,4))


# Obtenemos el score máximo, en k=15
scores[14]








from sklearn.metrics import accuracy_score


# Inicializamos el modelo con k=15
knn = KNeighborsClassifier(n_neighbors=15)

# Lo entrenamos con los datos de entrenamiento
knn.fit(X_train,y_train)

# Predecimos el valor de y
y_pred = knn.predict(X_test)

# Evaluamos la precisión del modelo
accuracy_score(y_test,y_pred)


# Otra manera de hacerlo
knn.score(X_test,y_test)





# Definimos df_pca como el dataFrame resultante de las componentes principales y la columna Pos_filt
df_pca = pd.DataFrame(componentesPrincipales,columns=["z1","z2"])
df_pca["Pos_filt"] = FBref["Pos_filt"]

# Visualizamos el resultado
df_pca


# Dividimos el conjunto el entrenamiento y testeo
df_pca_train, df_pca_test = train_test_split(df_pca,test_size=0.2,random_state=42)


# Definimos las variables explicativas y las variables respuesta
X_pca_train = df_pca_train.drop("Pos_filt",axis=1)
y_pca_train = df_pca_train["Pos_filt"]
X_pca_test = df_pca_test.drop("Pos_filt",axis=1)
y_pca_test = df_pca_test["Pos_filt"]


# Repetimos el proceso de evaluar los errores para los valores de k, ahora con df_pca
scores_pca = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X_pca_train, y_pca_train, cv=5)
    scores_pca.append(np.mean(score))


# Graficamos los resultados
(
    so.Plot(x=k_values,y=scores_pca)
    .add(so.Line())
    .add(so.Dot())
).layout(size=(6,4))


# Obtenemos el valor máximo, en k=17
scores_pca[16]





# Definimos un modelo de KNeighborsClasiffier con k=17
knn = KNeighborsClassifier(n_neighbors=17)

# Lo entrenamos con los datos de entrenamiento de las componentes principales
knn.fit(X_pca_train,y_pca_train)

# Predecimos la respuesta del conjunto de testeo
y_pca_pred = knn.predict(X_pca_test)

# Evaluamos la precisión del modelo
accuracy_score(y_pca_test,y_pca_pred)


# Utilizando la otra manera
knn.score(X_pca_test,y_pca_test)








df_fem = pd.read_csv("superleague2023.csv")
df_fem


# Eliminamos todas las columnas previas a MP
df_fem_num = df_fem.drop(df_fem.columns[0:7],axis=1)

# Verificamos el resultado
df_fem_num


# Como observamos que Min no es una columna numérica porque tiene comas, las eliminamos y la pasamos a int
df_fem_num["Min"] = df_fem_num["Min"].apply(lambda x:x.replace(",","")).astype("int64")


# Redefinimos data_num como los datos escalados
df_fem_num[df_fem_num.columns] = MinMaxScaler().fit_transform(df_fem_num)

# Observamos el resultado
df_fem_num


# Definimos Pos_filt de la misma manera que antes, es decir, quedandonos con la primera posición que aparece
df_fem["Pos_filt"]=df_fem["Pos"].apply(lambda x: x[:2])


# Definimos fem_clasif como los datos para la clasificación
fem_clasif = df_fem_num
fem_clasif["Pos_filt"] = df_fem["Pos_filt"]

# Visualizamos los resultados
fem_clasif


# Dividimos el DataFrame en datos de entrenamiento y testeo
fem_clasif_train, fem_clasif_test = train_test_split(fem_clasif,test_size=0.2,random_state=42)


# Definimos las variables predictoras y las predecidas
X_fem_train = fem_clasif_train.drop("Pos_filt",axis=1)
y_fem_train = fem_clasif_train["Pos_filt"]
X_fem_test = fem_clasif_test.drop("Pos_filt",axis=1)
y_fem_test = fem_clasif_test["Pos_filt"]


scores_fem = []


# Para cada valor de k, vamos guardando la media del score por validación cruzada en una lista scores_fem
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X_train, y_train, cv=5)
    scores_fem.append(np.mean(score))

# Graficamos el resultado
(
    so.Plot(x=k_values,y=scores_fem)
    .add(so.Line())
    .add(so.Dot())
).layout(size=(6,4))


# Obtenemos el maximo en k=15
scores_fem[16]





# Definimos un modelo de KNeighborsClasiffier con k=15
knn = KNeighborsClassifier(n_neighbors=15)

# Lo entrenamos con los datos de entrenamiento de las futbolistas mujeres
knn.fit(X_fem_train,y_fem_train)

# Predecimos la respuesta del conjunto de testeo
y_fem_pred = knn.predict(X_fem_test)

# Evaluamos la precisión del modelo
accuracy_score(y_fem_test,y_fem_pred)





scores_fem = []

# Extendemos el rango de los k en los que evaluamos el rendimiento, para ver si podemos encontrar alguno mejor
# for k in range(1,100):
#    knn = KNeighborsClassifier(n_neighbors=k)
#    score = cross_val_score(knn, X_train, y_train, cv=5)
#    scores_fem.append(np.mean(score))


# Graficamos los resultados
(
    so.Plot(x=range(1,100),y=scores_fem)
    .add(so.Line())
    .add(so.Dot())
).layout(size=(6,4))











# Cargamos el df y eliminamos una columna inútil
tmarket = pd.read_csv('transfermarkt_fbref_201920.csv',delimiter=';')
tmarket = tmarket.drop("Column1", axis = 1)
tmarket





display(tmarket.head(), tmarket.info())


# Veamos primero si hay algún dato faltante
with pd.option_context("display.max_rows",None):
    print(tmarket.isna().sum())


# Veamos ahora cuales son las columnas del tipo Object en nuestro df
columnas_object = tmarket.select_dtypes(include='object')

# Visualizamos dichas columnas
columnas_object


# Borramos columnas que no nos parecen relevantes para predecir el valor de un jugador, o ver cuál jugador es más parecido a otro
columnasAborrar = ["player", "nationality", "squad", "position2","foot", "league", "Attendance",
                       "Season", "W", "D", "L", "MP", "CL", "WinCL", "birth_year", "CLBestScorer"]
df_num = tmarket.drop(columnasAborrar, axis = 1)

# Nos aseguramos que queden sólo variables numéricas
df_num = df_num.select_dtypes(include=["int64", "float64"])


# Escalamos los datos al intervalo [0,1]
df_num[df_num.columns] = MinMaxScaler().fit_transform(df_num)





# Obtenemos una rápida visualización de Messi, para conocerlo un poco
tmarket[tmarket["player"] == "Lionel Messi"]


# Definimos una función que nos devuelva los primeros 10 jugadores más parecidos para un jugador determinado
def jugadores_recomendados (nombre_jugador, cant_recomendaciones):
    # Obtenemos el índice del jugador
    ind_jugador = tmarket[tmarket["player"]==nombre_jugador].index[0]

    # Inicializamos un NearestNeighbors que busque los n+1 jugadores más parecidos al buscado (porque incluye al jugador buscado)
    neighbors = NearestNeighbors(n_neighbors = cant_recomendaciones+1).fit(df_num)

    # Obtenemos las distancias a esos jugadores y sus índices
    distances, indices = neighbors.kneighbors(df_num.iloc[[ind_jugador]])

    # Devolvemos el dataset evaluado en esos índices
    res = tmarket.iloc[indices.flatten()]

    # Eliminamos de la lista al mismo jugador
    res = res.drop(ind_jugador, axis=0)
    
    return res


jugadores_recomendados("Lionel Messi", 10)








tmarket.head()


data = tmarket.drop(columnasAborrar, axis =1)
data = data.select_dtypes(include=["float64","int64"])
data
# Acá elijo con qué columnas me voy a quedar


X = data.drop(columns=['value']) # Las columnas con las que vamos a predecir
y = data['value']                # La columna a predecir


# Separamos en conjuntos de train y test, luego escalamos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)
scaler_X = MinMaxScaler().set_output(transform="pandas")
scaler_y = MinMaxScaler().set_output(transform="pandas")
X_train = scaler_X.fit_transform(X_train)
X_test = scaler_X.transform(X_test)
y_train = scaler_y.fit_transform(pd.DataFrame(y_train))
y_test = scaler_y.transform(pd.DataFrame(y_test))


keras.utils.set_random_seed(9)

# Paso 1: iniciamos el modelo indicando la arquitectura de la red
model = keras.Sequential([
    keras.layers.Input(shape=(X_train.shape[1],)), # Seleccionamos la dim del input
    keras.layers.Dense(200,                          # Selccionamos la dim de la 1er capa
                       activation='sigmoid'),# Función de activación de la 1er capa
    keras.layers.Dense(100,
                       activation="tanh"),
    keras.layers.Dense(1,                          # Seleccionamos la dim de la 2da capa, y debe coincidir con la "y" a predecir
                       activation='sigmoid')       # Función de activación de la 2da capa
])

# Paso 2: configuramos el optimizador
optimizer = keras.optimizers.SGD(learning_rate=0.5) # Elegimos el learning rate

# Paso 3: construimos el modelo
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',  # Elegimos nuestra función de error
)

# Paso 4: entrenamiento del modelo.
hist = model.fit(X_train.to_numpy(), y_train.to_numpy(), # Le pasamos nuestros datos de train
                 epochs=100,                             # Elegimos la cantidad de epocas que queremos
                 batch_size=20,                          # Elegimos el tamaño del batch
                 validation_split=0.2,                   # Elegimos el tamaño del validation_split
                 verbose=0,                              # Oculta todo el proceso
                 )

# Paso 5: calculamos el error en el conjunto de testeo
model.evaluate(X_test.to_numpy(), y_test.to_numpy(),    # Le pasamos los datos de test
               verbose=0,                               # Ocultamos el proceso
               batch_size=len(y_test),                  # Un solo batch
               return_dict=True
               )








# Definimos escaladore para X e y
scaler_X = MinMaxScaler().set_output(transform="pandas")
scaler_y = MinMaxScaler().set_output(transform="pandas")
X = scaler_X.fit_transform(X)
y = scaler_y.fit_transform(pd.DataFrame(y))


hist = model.fit(X.to_numpy(), y.to_numpy(), # Le pasamos nuestros datos de train
                 epochs=100,                             # Elegimos la cantidad de epocas que queremos
                 batch_size=20,                          # Elegimos el tamaño del batch
                 validation_split=0.2,                   # Elegimos el tamaño del validation_split
                 verbose=0,                              # Oculta todo el proceso
                 )



# Obtenemos la predicción de nuestro modelo del valor de los jugadores, escalada
pred = model.predict(X.to_numpy()).flatten()

# La "desescalamos"
pred = scaler_y.inverse_transform(pd.DataFrame(pred))

# Mostramos los resultados
pred


# Creamos una nueva columna value_pred donde guardaremos el valor que nuestro modelo le asigno a cada jugador
data["value_pred"] = pred.flatten()

# Mostramos los resultados
data


# Definimos la columna dif que nos dice la diferencia entre en valor real y el valor predicho
data["dif"] = data["value"] - data["value_pred"]

# Mostramos los resultados
data


# Obtenemos el índice del jugador más sobrevalorado
data["dif"].idxmax()


# Obtenemos al jugador más sobrevalorado del mundo
tmarket.iloc[data["dif"].idxmax()]["player"]


# Obtenemos al jugador más infravalorado del mundo
tmarket.iloc[data["dif"].idxmin()]["player"]





# Observamos los datos de Mbappé para tenerlos como referencia
tmarket[tmarket["player"]=="Kylian Mbappé"] 


# Definimos recomendacion como el DataFrame que tiene a los 10 jugadores más parecidos a Mbappé
recomendacion = jugadores_recomendados("Kylian Mbappé",10)

# Mostramos los resultados
recomendacion


# Obtenemos el valor que nuestro modelo predice para cada jugador de nuestra lista
recomendaciones_pred = data.iloc[recomendacion.index]["dif"]
recomendaciones_pred


# Obtenemos el id del jugador más "infravalorado"
recomendaciones_pred.idxmin()


# El jugador parecido a Mbappé con mejor relación calidad-precio
recomendacion.loc[274]["player"]















